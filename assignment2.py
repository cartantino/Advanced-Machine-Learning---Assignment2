# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1erZrDfBT_WF7uCiL1r1lK7jvYG_qUClp

# **Assignment 2**

# Load and preprocess data

### Loading test and train
"""

import pickle

import numpy as np

from sklearn.model_selection import train_test_split

from keras.utils import np_utils
from keras.utils import to_categorical
from keras.layers import Input, Dense
from keras import regularizers
from keras.models import Model

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.callbacks import EarlyStopping

from matplotlib import pyplot as plt

with open('x_test.obj', 'rb') as xtest:
  test_set = pickle.load(xtest)
  xtest.close()
  
with open('x_train.obj', 'rb') as xtrain:
  X_set = pickle.load(xtrain)
  xtrain.close()
  
with open('y_train.obj', 'rb') as ytrain:
  Y_train = pickle.load(ytrain)
  ytrain.close()

"""### Split training set into train and validation
***train*** = 80% & ***validation*** = 20%
"""

training_set, validation_set, training_labels, validation_labels = train_test_split(X_set, Y_train, test_size=0.2, shuffle = 'True', random_state = 42)

"""### Preprocess data"""

training_set = training_set.astype('float32') / 255.
test_set = test_set.astype('float32') / 255.
validation_set = validation_set.astype('float32') / 255.



training_set = training_set.reshape((len(training_set), np.prod(training_set.shape[1:])))
validation_set = validation_set.reshape((len(validation_set), np.prod(validation_set.shape[1:])))
test_set = test_set.reshape((len(test_set), np.prod(test_set.shape[1:])))

fold_train_set = training_set
fold_val_set = validation_set


training_labels = training_labels - 16
validation_labels = validation_labels - 16


fold_train_labels = training_labels
fold_val_labels = validation_labels

training_labels = to_categorical(training_labels, 11)
validation_labels = to_categorical(validation_labels, 11)

print(fold_val_labels)

"""# Building the autoencoder

## Simple Auto Encoder
"""

# this is the size of our encoded representations
encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats

# We got 784 columns, one columns for each pixel
print(training_set.shape[1])

# this is our input placeholder
input_img = Input(shape=(784,))
# "encoded" is the encoded representation of the input
encoded = Dense(encoding_dim, activation='relu')(input_img)

# "decoded" is the lossy reconstruction of the input
decoded = Dense(784, activation='sigmoid')(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
encoded_input = Input(shape=(encoding_dim,))

# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]

# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')
autoencoder.summary()

#note: x_train, x_train :) 
autoencoder.fit(training_set, training_set,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(validation_set, validation_set))

"""### Checking reconstruction"""

encoded_imgs = encoder.predict(validation_set)
decoded_imgs = decoder.predict(encoded_imgs)

n = 10 
plt.figure(figsize=(20, 4))
for i in range(n):
    # original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(validation_set[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

"""## Modify the Autoencoder

### Adding a sparsity constraint on the encoded representations
In the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would "fire" at a given time. In Keras, this can be done by adding an activity_regularizer to our Dense layer:
"""

# this is the size of our encoded representations
encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats

# We got 784 columns, one columns for each pixel
print(training_set.shape[1])

# this is our input placeholder
input_img = Input(shape=(784,))


# Early stopping
callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10)]

# "encoded" is the encoded representation of the input
encoded = Dense(encoding_dim, activation='relu',
               activity_regularizer=regularizers.l1(10e-9))(input_img)

# "decoded" is the lossy reconstruction of the input
decoded = Dense(784, activation='sigmoid')(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
encoded_input = Input(shape=(encoding_dim,))

# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]

# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics = ['accuracy'])
autoencoder.summary()

autoencoder.fit(training_set, training_set,
                callbacks = callbacks,
                epochs=300,
                batch_size=200,
                shuffle=True,
                validation_data=(validation_set, validation_set))

from matplotlib import pyplot as plt

encoded_imgs = encoder.predict(validation_set)
decoded_imgs = decoder.predict(encoded_imgs)

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(validation_set[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

"""## Final Autoencoder"""

input_img = Input(shape=(784,))
encoded = Dense(512, activation = 'relu')(input_img)
encoded = Dense(128, activation = 'relu')(encoded)
encoded = Dense(64, activation = 'relu')(encoded)
encoded = Dense(32, activation = 'relu')(encoded)

decoded = Dense(64, activation = 'relu')(encoded)
decoded = Dense(128, activation = 'relu')(decoded)
decoded = Dense(512, activation = 'relu')(decoded)
decoded = Dense(784, activation = 'sigmoid')(decoded)

autoencoder = Model(input_img, decoded)

encoding_dim = 32

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
encoded_input = Input(shape=(encoding_dim,))

# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-4]

# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])
autoencoder.summary()

encoder.summary()

callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)]
net_history = autoencoder.fit(training_set, training_set,
                epochs=100,
                batch_size=256,
                callbacks = callbacks,
                shuffle=True,
                validation_data=(validation_set, validation_set))

#encoded_imgs = encoder.predict(validation_set)
decoded_imgs = autoencoder.predict(validation_set)

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(validation_set[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

n_epochs = len(net_history.history['loss'])
#n_epochs = 21
x_plot = list(range(1,n_epochs+1))

def plot_history(network_history):
    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.plot(x_plot, network_history.history['loss'])
    plt.plot(x_plot, network_history.history['val_loss'])
    plt.legend(['Training', 'Validation'])

plot_history(net_history)

"""# Neural Network with encoder

### NN
"""

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.callbacks import EarlyStopping
from keras import regularizers

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
import tensorflow as tf


# define 10-fold cross validation
kfold = KFold(n_splits=10, shuffle=True, random_state=42)
scores_train = []
scores_val = []
best_score = 0  # based on recall
best_model = None
best_history = None

#define variable to store results
import numpy as np
accuracy_results = np.zeros((10,2))

for idx, (train, test) in enumerate(kfold.split(training_set, training_labels)):
  print('FOLD: ', idx + 1)
  # create model
 
  # l2 penalizza pesi alti che prevalgono sugli altri e non me li porta a zero, l1 invece tende a portare a zero i neuroni che hanno peso basso

  model = Sequential()
  # First Layer is simply the encoder we trained above
  model.add(encoder)
  model.add(Dense(32, activation = 'relu'))
  model.add(Dropout(0.1))
  model.add(Dense(16, activation = 'relu'))
  model.add(Dense(11, activation = 'softmax'))

  # The weights of the encoder are freezed
  model.layers[0].trainable = False

  model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])

  callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)]
  # Fit the model
  history = model.fit(
    training_set[train], training_labels[train], 
    epochs=200,
    validation_data=(training_set[test], training_labels[test]),
    callbacks = callbacks,
    batch_size=128,
    shuffle=True
  )
  # evaluate the model
  score = model.evaluate(training_set[test], training_labels[test], verbose=1)
  
  for i in range(2):
    accuracy_results[idx][i] = score[i] 
    
  # save best_model
  if score[1] > best_score:
    best_score = score[1]
    print(score[1])
    best_model = model
    best_history = history
  print('Performance on the validation set')
  for idx in range(len(model.metrics_names)):
    print("%s: %.2f" % (model.metrics_names[idx], score[idx]))
  scores_val.append(score)
  print()

accuracy_results

"""## Metrics"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

n_epochs = len(best_history.history['loss'])
x_plot = list(range(1,n_epochs+1))

def plot_history(best_history):
    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.plot(x_plot, best_history.history['loss'])
    plt.plot(x_plot, best_history.history['val_loss'])
    plt.legend(['Training', 'Validation'])

    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.plot(x_plot, best_history.history['acc'])
    plt.plot(x_plot, best_history.history['val_acc'])
    plt.legend(['Training', 'Validation'], loc='lower right')
    plt.show()

plot_history(best_history)

from sklearn.metrics import confusion_matrix
predicted = best_model.predict_classes(validation_set)
predicted = predicted + 16
print(predicted)

from sklearn.metrics import classification_report 
 
predict = np.round(best_model.predict(validation_set))

targets = ['P','Q','R','S','T','U','V','W','X','Y', 'Z']
print(classification_report(predict, validation_labels , target_names = targets))

from mlxtend.evaluate import confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix

predict = best_model.predict_classes(validation_set)
predict = predict + 16
print(predict)
fold_val_labels = fold_val_labels + 16
print(fold_val_labels)
cm = confusion_matrix(y_target = fold_val_labels, 
                      y_predicted = predict, 
                      binary=False)


fig, ax = plot_confusion_matrix(conf_mat=cm)
plt.show()

"""## Check reconstruction, again.. if we have the same decoding images as above then the decoder has no changed weights."""

encoded_imgs = encoder.predict(validation_set)
decoded_imgs = autoencoder.predict(validation_set)

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(validation_set[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

"""# Traditional neural network w 10 fold cross validation

## 10 Fold cross validation
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
import tensorflow as tf



# define 10-fold cross validation
kfold = KFold(n_splits=10, shuffle=True, random_state=42)
scores_train = []
scores_val = []
best_score = 0  # based on recall
best_model = None
best_history = None

#define variable to store results
import numpy as np
accuracy_results = np.zeros((10,2))

for idx, (train, test) in enumerate(kfold.split(training_set, training_labels)):
  print('FOLD: ', idx + 1)
  # create model
 
  model = Sequential()
  model.add(Dense(784, input_shape=(training_set.shape[1],), activation = 'relu'))
  model.add(Dropout(rate = 0.4))
  model.add(Dense(512, activation='relu'))
  model.add(Dropout(rate = 0.4))
  model.add(Dense(256, activation='relu'))
  model.add(Dropout(rate = 0.4))
  model.add(Dense(128, activation = 'relu'))
  model.add(Dropout(rate = 0.3))
  model.add(Dense(64, activation = 'relu'))
  model.add(Dropout(rate = 0.2))
  model.add(Dense(32, activation = 'relu'))
  model.add(Dropout(rate = 0.2))
  model.add(Dense(11, activation = 'softmax'))
  model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])
  callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)]
  # Fit the model
  history = model.fit(
    training_set[train], training_labels[train], 
    epochs=200,
    validation_data=(training_set[test], training_labels[test]),
    callbacks = callbacks,
    batch_size=128,
    shuffle=True
  )
  # evaluate the model
  score = model.evaluate(training_set[test], training_labels[test], verbose=1)
  
  for i in range(2):
    accuracy_results[idx][i] = score[i] 
    
  # save best_model
  if score[1] > best_score:
    best_score = score[1]
    print(score[1])
    best_model = model
    best_history = history
  print('Performance on the validation set')
  for idx in range(len(model.metrics_names)):
    print("%s: %.2f" % (model.metrics_names[idx], score[idx]))
  scores_val.append(score)
  print()

standard_nn_10_fold = accuracy_results
standard_nn_10_fold

"""## Best model metrics"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

net_history = best_history
n_epochs = len(net_history.history['loss'])
x_plot = list(range(1,n_epochs+1))

def plot_history(net_history):
    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.plot(x_plot, net_history.history['loss'])
    plt.plot(x_plot, net_history.history['val_loss'])
    plt.legend(['Training', 'Validation'])

    plt.figure()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.plot(x_plot, net_history.history['acc'])
    plt.plot(x_plot, net_history.history['val_acc'])
    plt.legend(['Training', 'Validation'], loc='lower right')
    plt.show()

plot_history(net_history)

from sklearn.metrics import classification_report 
 
predict = np.round(best_model.predict(validation_set))

targets = ['P','Q','R','S','T','U','V','W','X','Y', 'Z']
print(classification_report(predict, validation_labels , target_names = targets))

"""## Confusion matrix of the best model"""

from mlxtend.evaluate import confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix

predict = best_model.predict_classes(validation_set)
predict = predict + 16

fold_val_labels = fold_val_labels + 16

cm = confusion_matrix(y_target = fold_val_labels, 
                      y_predicted = predict, 
                      binary=False)

fig, ax = plot_confusion_matrix(conf_mat=cm)
plt.show()

"""# Prediction on test set"""

predict = best_model.predict_classes(validation_set)
predict = predict + 16

print(predict)

"""## Creation of the txt file with results"""

with open("Carta_Costantino_808417_score2.txt", "w") as f:
  for i in predict:
    f.write("%s\n" % str(i))